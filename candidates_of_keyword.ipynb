{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidates of keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import spacy\n",
    "import json\n",
    "# import flashtext\n",
    "from pl_module import get_pl_keywords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get NOUN TRUNK key candidates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getN_Trunk(doc):\n",
    "    candidates = []\n",
    "    doc = nlp(doc)\n",
    "    chunk = list(doc.noun_chunks)\n",
    "    for token in chunk:\n",
    "        candidates.append(str(token))\n",
    "        \n",
    "    return candidates\n",
    "\n",
    "def getoffStopWord(n_chucks):\n",
    "    for n,i in enumerate(n_chucks):\n",
    "        n_str = list(i)\n",
    "        for m,word in enumerate(n_str):\n",
    "            word = str(word)\n",
    "            if word.lower() in stopwords.words('english'):\n",
    "                n_str.remove(n_str[m])\n",
    "            else:\n",
    "                n_str[m]=str(word)\n",
    "        n_chucks[n]=n_str\n",
    "    \n",
    "    \n",
    "def getoffDash(n_trunks):\n",
    "    for n,i in enumerate(n_trunks):\n",
    "        i = str(i)\n",
    "        n_str = []\n",
    "        if i.startswith( '-' ):\n",
    "            n_str=list(i)\n",
    "            if n_str[1]==\" \":\n",
    "                n_str[1]=\"\"\n",
    "            n_trunks[n] = \"\".join(n_str)\n",
    "                \n",
    "    return n_trunks\n",
    "\n",
    "def get_NT(chucks):\n",
    "    candidates =[]\n",
    "    for n,nt in enumerate(chucks):\n",
    "        nt = str(nt)\n",
    "        # get off stopword\n",
    "        words = nt.split(\" \") # NT\n",
    "        for m, word in enumerate(words): # check stopword in NT\n",
    "            word = str(word)\n",
    "            if word.lower() in stopwords.words('english'):\n",
    "                words.remove(word)\n",
    "            else:\n",
    "                words[m]=str(word)\n",
    "        nt = \" \".join(words)\n",
    "        \n",
    "        \n",
    "        # get of strange startword\n",
    "        n_str = []\n",
    "        while nt.startswith('-') or nt.startswith(\"\\n\") \\\n",
    "        or nt.startswith(\" \") or nt.startswith(\"\\t\") or nt.startswith(\":\"):\n",
    "            n_str=list(nt) # split into character\n",
    "            n_str.remove(n_str[0])\n",
    "            nt=\"\".join(n_str)\n",
    "        \n",
    "\n",
    "        if nt:\n",
    "            candidates.append(\"\".join(nt))\n",
    "        \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n"
     ]
    }
   ],
   "source": [
    "json_strucure=[\"jobTitle\",\"jobEmployer\",\"jobLocation\",\"jobPostTime\",\"skills\",\\\n",
    "               \"employmentType\",\"baseSalary\",\"jobDescription\",\"url\"]\n",
    "    \n",
    "js_data = json.load(open('../Steeve_data/Backend-NY-390.json'))\n",
    "\n",
    "ori_data = js_data[\"Backend-NY\"]\n",
    "\n",
    "data = {}  \n",
    "\n",
    "data[\"Backend-NY\"] = []\n",
    "\n",
    "print(\"load data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine NT and PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n"
     ]
    }
   ],
   "source": [
    "# for job_num in range(1):   \n",
    "for job_num in range(len(ori_data)):\n",
    "#     print(job_num)\n",
    "    data_des = ori_data[job_num][\"jobDescription\"]\n",
    "    nt_des = nlp(data_des)\n",
    "    nt_des = list(nt_des.noun_chunks)\n",
    "    can_description = get_NT(nt_des)\n",
    "    \n",
    "    data_ski = ori_data[job_num][\"skills\"]\n",
    "    nt_ski = nlp(data_ski)\n",
    "    nt_ski = list(nt_ski.noun_chunks)\n",
    "    can_skills = get_NT(nt_ski)\n",
    "    \n",
    "    pl_des = get_pl_keywords(data_des)\n",
    "    pl_ski = get_pl_keywords(data_ski)\n",
    "    \n",
    "    data[\"Backend-NY\"].append({  \n",
    "    \"jobTitle\": ori_data[job_num][\"jobTitle\"],\n",
    "    \"NT\": can_description+can_skills,\n",
    "    \"PL\": pl_des+pl_ski, \n",
    "    \"url\": ori_data[job_num][\"url\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Json.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Steeve_data/NounTrunks-NY.txt', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Job Requisition Number', '57546 \\n\\nOur Team', \"Bloomberg's Trade Automation & Execution team\", 'multiple execution management', 'automation solutions', 'products', 'financial professionals', 'integrated multi-asset class trading platform', 'order lifecycle', 'real-time analytics', 'thousands broker destinations', 'trading venues', 'products', 'trading functionality', 'wide range', 'asset classes', 'equities', 'options', 'futures', 'bonds', 'FX', 'repos', 'mortgages', 'derivatives', 'than trillion dollars', 'systems', 'business', 'trends', 'trade automation', 'predictive analytics', 'higher data volumes', 'increased focus', 'speed', 'multiple exciting initiatives', 'order', 'needs', 'next generation state', 'art execution platform', 'front-row seat', 'financial markets', 'insights', 'perspectives', 'software', 'future direction', 'high-performance, fault-tolerant, real-time systems', 'goal', 'millions', 'transactions', 'chance', 'multiple industry-leading technologies', 'Cassandra', 'Apache Storm', 'Kafka', 'WebSphere MQ', 'Redis', 'new services', 'C++11', 'Linux', 'work', 'diverse global community', 'traders', 'brokers', '2+ years', 'experience programming', 'C', 'C++', 'UNIX/Linux \\n\\n robust knowledge', 'data structures', 'algorithms \\n\\n Experience', 'multi-threading asynchronous event-driven programming \\n\\n BA/BS', 'MA/MS', 'Computer Science', 'Engineering', 'equivalent professional experience', 'desire', 'willingness', 'creative solutions', 'tough engineering problems', 'interest', 'financial markets', 'Experience', 'high volume', 'high availability', 'systems \\n\\n Experience', 'distributed middleware', 'WebSphere MQ', 'RabbitMQ', 'Kafka', 'Familiarity', 'scripting language', 'Python', 'developer productivity tools', 'Jenkins', 'Coverity', 'Splunk', 'challenges', 'passion', 'technology', 'flexible, creative approach', 'problem', 'application', 'Date', 'Tue', '07 11 2017 00:00:00 GMT \\nDepartment', 'Software Developer/Engineering', 'Algorithms', 'Apache', 'C++', 'Developer', 'Jenkins', 'Lifecycle', 'Linux', 'Management', 'Middleware', 'Programming', 'Python', 'Software Engineer', 'Unix', 'WebSphere']\n",
      "['automation', 'automation', 'class', 'order', 'real-time', 'Connect', 'build', 'range', 'automation', 'focus', 'stack', 'order', 'build', 'get', 'row', 'build', 'get', 'performance', 'real-time', 'process', 'transactions', 'get', 'cassandra', 'apache', 'websphere', 'redis', 'c++11', 'linux', 'c', 'c++', 'UNIX', 'linux', 'asynchronous', 'websphere', 'rabbitmq', 'scripting', 'python', 'jenkins', 'Splunk', 'submit', 'date', 'apache', 'c++', 'jenkins', 'linux', 'python', 'UNIX', 'websphere']\n"
     ]
    }
   ],
   "source": [
    "asd = json.load(open('../Steeve_data/NounTrunks-NY.txt'))\n",
    "\n",
    "text = asd[\"Backend-NY\"]\n",
    "\n",
    "print(text[0][\"NT\"])\n",
    "print(text[0][\"PL\"])\n",
    "# print(ori_data[26][\"skills\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
