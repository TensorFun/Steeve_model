{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidates of keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('compatibility', 1.0), ('of', 1.0), ('systems', 1.0), ('linear', 1.0), ('constraints', 1.0), ('over', 1.0), ('the', 1.0), ('set', 1.0), ('natural', 1.0), ('numbers', 1.0), ('criteria', 1.0), ('system', 1.0), ('diophantine', 1.0), ('equations', 1.0)]\n",
      "[('minimal generating sets', 8.666666666666666), ('linear diophantine equations', 8.5), ('minimal supporting set', 7.666666666666666), ('minimal set', 4.666666666666666), ('linear constraints', 4.5), ('natural numbers', 4.0), ('strict inequations', 4.0), ('nonstrict inequations', 4.0), ('upper bounds', 4.0), ('mixed types', 3.666666666666667), ('considered types', 3.166666666666667), ('set', 2.0), ('types', 1.6666666666666667), ('considered', 1.5), ('compatibility', 1.0), ('systems', 1.0), ('criteria', 1.0), ('system', 1.0), ('components', 1.0), ('solutions', 1.0), ('algorithms', 1.0), ('construction', 1.0), ('constructing', 1.0), ('solving', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import spacy\n",
    "import json\n",
    "import os\n",
    "# from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from rake import Rake\n",
    "from pl_module import get_pl_keywords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nlp = spacy.load('en',disable=['ner', 'tagger'])\n",
    "PATH = '../Steeve_data/no_filter_Dice/raw_data_post/'\n",
    "eng_stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Data_loading(PATH):\n",
    "    \n",
    "    # load json files and get title name of each json file\n",
    "    file_data=[]\n",
    "    feild_names=[]\n",
    "    key = []\n",
    "    for path, dirs, files in os.walk(PATH):\n",
    "        for i,file in enumerate(files):\n",
    "            file_data.append( json.load(open(PATH+file)))\n",
    "            for k in file_data[i].keys():\n",
    "                key.append(k)\n",
    "            \n",
    "    return file_data,key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get key candidates \n",
    "#### Get NC(spaCy), NC(rake) and programming languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getN_Crunk(doc):\n",
    "    \n",
    "    candidates = []\n",
    "    doc = nlp(doc)\n",
    "    chunk = list(doc.noun_chunks)\n",
    "    candidates = list(map(lambda el: str(el).lower(), chunk))\n",
    "    \n",
    "#     for token in chunk:\n",
    "#         candidates.append(token.lower())\n",
    "        \n",
    "    return candidates\n",
    "\n",
    "\n",
    "def getoffStopWord(n_chunks):\n",
    "    \n",
    "    for n, n_str in enumerate(n_chunks):\n",
    "\n",
    "        n_s = n_str.split(\" \")\n",
    "        for word in n_s:\n",
    "            if word in eng_stop:\n",
    "                n_s.remove(word)\n",
    "\n",
    "        n_chunks[n] = \" \".join(n_s)\n",
    "    \n",
    "    return n_chunks\n",
    "    \n",
    "def getCleanWord(n_chunks):\n",
    "#     exclude = \"(\\w+)( \\-|\\- | \\- )(\\w+)\"\n",
    "    exclude=\"[\\w ]|[ \\w][\\w\\-\\w]\"\n",
    "#     exclude = \"[(\\w+)-(\\w+)]\"\n",
    "    \n",
    "    for n, n_str in enumerate(n_chunks):\n",
    "        n_str = n_str.replace(\"\\n\", \" \")\n",
    "        matches = re.findall(exclude,n_str)\n",
    "        cl_words = \"\".join(matches)\n",
    "        n_chunks[n] = cl_words\n",
    "                \n",
    "    return n_chunks\n",
    "\n",
    "def get_NC(n_chunks):\n",
    "    \n",
    "    n_chunks = getCleanWord(n_chunks)\n",
    "    n_chunks = getoffStopWord(n_chunks)\n",
    "    \n",
    "    return n_chunks\n",
    "\n",
    "def get_Rake_NC(n_chunks):\n",
    "    \n",
    "    rake_object = Rake(\"SmartStoplist.txt\")\n",
    "    keywords = rake_object.run(n_chunks)\n",
    "    rake_skills = [x[0] for x in keywords]\n",
    "    \n",
    "    return rake_skills\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "backend\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "andriod\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "frontend\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "security\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ori_data, field_names = Data_loading(PATH)\n",
    "#     json_strucure=[\"jobTitle\",\"jobEmployer\",\"jobLocation\",\"jobPostTime\",\"skills\",\\\n",
    "#                \"employmentType\",\"baseSalary\",\"jobDescription\",\"url\"]\n",
    "     \n",
    "    print(\"load data\")\n",
    "    sentences = ''\n",
    "    rake_object = Rake(\"SmartStoplist.txt\")\n",
    "    \n",
    "    for i, f in enumerate(ori_data):  \n",
    "        print(field_names[i])\n",
    "        data = {} \n",
    "        data[field_names[i]] = []\n",
    "    \n",
    "        for num, job_num in enumerate(ori_data[i][field_names[i]]):\n",
    "            if num%500 ==0:\n",
    "                print(num)\n",
    "            ### testing gensim\n",
    "#                 sentences += job_num[\"jobDescription\"]\n",
    "            ### testing gensim\n",
    "            \n",
    "            nc_des = getN_Crunk(job_num[\"jobDescription\"])\n",
    "            nc_ski = getN_Crunk(job_num[\"skills\"])\n",
    "\n",
    "            can_description = get_NC(nc_des)\n",
    "            can_skills = get_NC(nc_ski)\n",
    "            \n",
    "            rake_des = get_Rake_NC(job_num[\"jobDescription\"])\n",
    "            rake_ski = get_Rake_NC(job_num[\"skills\"])        \n",
    "            \n",
    "            pl_des = get_pl_keywords(job_num[\"jobDescription\"])\n",
    "            pl_ski = get_pl_keywords(job_num[\"skills\"])\n",
    "            \n",
    "            data[field_names[i]].append({\n",
    "                \"jobTitle\": job_num[\"jobTitle\"],\n",
    "                \"NC\": can_description+ can_skills+rake_des+rake_ski,\n",
    "                \"PL\": pl_des+ pl_ski, \n",
    "                \"url\": job_num[\"url\"]})\n",
    "        \n",
    "        with open('../Steeve_data/no_filter_Dice/can/Keywords'+field_names[i]+'.txt', 'w') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "\n",
    "    print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asd = json.load(open('../Steeve_data/candidates_keyword/KeywordsBackend.txt'))\n",
    "\n",
    "text = asd[\"Backend\"]\n",
    "\n",
    "testing_data = ori_data[0][field_names[0]][1][\"jobDescription\"]\n",
    "# print(testing_data)\n",
    "\n",
    "# print(text[123][\"NC\"])\n",
    "# for i in text[0][\"NC\"]:\n",
    "#     print(i)\n",
    "# print(ori_data[26][\"skills\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
