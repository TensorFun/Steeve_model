{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, OrderedDict\n",
    "import os\n",
    "from operator import itemgetter    \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from TFIDF import TFIDF\n",
    "import requests\n",
    "\n",
    "DNN_model_meta_path = '../dnn_model.ckpt.meta'\n",
    "DNN_model_ck_path = '../dnn_model.ckpt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v model\n",
    "from pyfasttext import FastText\n",
    "model = FastText('/home/vincent/atos/wiki.en.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get post features 2400-d\n",
    "def get_pl_v(j,pl_cnt,NUM_PL,D_WORD):\n",
    "    m = []\n",
    "    aaa = {}\n",
    "    \n",
    "    # compare four fields tf-idf value\n",
    "    N_SCORE = .0\n",
    "    MAX_SCORE = .0\n",
    "    PREDICT_FIELD = '' \n",
    "    \n",
    "    for cnt_f in pl_cnt:\n",
    "        for pl in j:\n",
    "            if pl_cnt[cnt_f].get(pl) == None:\n",
    "                continue\n",
    "                N_SCORE += pl_cnt[cnt_f].get(pl)\n",
    "            if N_SCORE > MAX_SCORE:\n",
    "                PREDICT_FIELD = cnt_f\n",
    "                MAX_SCORE = N_SCORE\n",
    "            elif PREDICT_FIELD == '':\n",
    "                PREDICT_FIELD = cnt_f\n",
    "                        \n",
    "    for pl in j:\n",
    "#         print(type(pl))\n",
    "        if pl_cnt[PREDICT_FIELD].get(pl) == None:\n",
    "            pass\n",
    "        else:\n",
    "            aaa[pl] = pl_cnt[PREDICT_FIELD].get(pl)\n",
    "            \n",
    "    for i, g in sorted(dict(aaa).items(), key=itemgetter(1), reverse=True)[:8]:\n",
    "        try:\n",
    "            x = list(model[i])\n",
    "            m = m + x\n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "                \n",
    "    if len(m) < D_WORD*NUM_PL:\n",
    "        if (len(m)/D_WORD)%2 == 1:\n",
    "            m = m+m[0:D_WORD]\n",
    "        if len(m) == D_WORD*6:\n",
    "            m = m + m[0:D_WORD*2]\n",
    "        elif len(m) == D_WORD*4:\n",
    "            m = m + m\n",
    "        elif len(m) == D_WORD*2:\n",
    "                m = m + m[0:D_WORD]\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl_preprocessing(total_pl):\n",
    "    train_data = []\n",
    "    train_y = []\n",
    "    NUM_PL = 8\n",
    "    D_WORD = 300\n",
    "    tf_idf = TFIDF(total_pl)\n",
    "    pl_cnt, words = tf_idf.get_tfidf()\n",
    "    \n",
    "    # label\n",
    "    l = 0\n",
    "    \n",
    "    for field in total_pl:\n",
    "#         print(field)\n",
    "        for num, j in enumerate(field):\n",
    "\n",
    "            m = get_pl_v(j,pl_cnt,NUM_PL,D_WORD)\n",
    "            if len(m) == 2400:                    \n",
    "                train_data.append(m)\n",
    "                train_y.append(l)\n",
    "            else:\n",
    "                pass\n",
    "        l += 1\n",
    "#                             print(i)\n",
    "#     print(t,s)\n",
    "            \n",
    "    return train_data,train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 額外程式，主要去打亂合併的資料，並做k-fold 的取資料\n",
    "class CrossValidationFolds(object):\n",
    "    \n",
    "    def __init__(self, data, labels, num_folds, shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.num_folds = num_folds\n",
    "        self.current_fold = 0\n",
    "        \n",
    "        # Shuffle Dataset\n",
    "        if shuffle:\n",
    "            perm = np.random.permutation(self.data.shape[0]) ##隨機打亂資料\n",
    "            data = data[perm]\n",
    "            labels = labels[perm]\n",
    "    \n",
    "    def split(self):\n",
    "        current = self.current_fold\n",
    "        size = int(self.data.shape[0]/self.num_folds) # 30596 / 5 一塊k的size大小\n",
    "        \n",
    "        index = np.arange(self.data.shape[0]) \n",
    "\n",
    "        # 利用 True/False 抓出 validation 區塊\n",
    "        lower_bound = index >= current*size # validation 下界\n",
    "        upper_bound = index < (current + 1)*size # 上界\n",
    "\n",
    "        cv_region = lower_bound*upper_bound\n",
    "\n",
    "        cv_data = self.data[cv_region] # 利用 True/False 抓出 True 的資料\n",
    "        train_data = self.data[~cv_region]\n",
    "        \n",
    "        cv_labels = self.labels[cv_region]\n",
    "        train_labels = self.labels[~cv_region]\n",
    "        \n",
    "        self.current_fold += 1 ## 丟回下一的fold\n",
    "        return (train_data, train_labels), (cv_data, cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layers_model(X, h_units, n_class, dropout=0.5):\n",
    "    # default he_init: factor=2.0, mode='FAN_IN', uniform=False, seed=None, dtype=tf.float32\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    with tf.name_scope(\"DNN\"):\n",
    "        hidden1 = tf.layers.dense(X, 128, activation=tf.nn.relu, name=\"hidden1\",use_bias=True, kernel_initializer= he_init,\n",
    "                                      bias_initializer=he_init)\n",
    "        dropout1= tf.layers.dropout(hidden1, rate=0.5,name=\"dropout1\")\n",
    "        \n",
    "        hidden2 = tf.layers.dense(dropout1, 128, activation=tf.nn.relu, name=\"hidden2\",use_bias=True, kernel_initializer= he_init,\n",
    "                                      bias_initializer=he_init)\n",
    "        dropout2= tf.layers.dropout(hidden2, rate=0.5,name=\"dropout2\")\n",
    "        \n",
    "        hidden3 = tf.layers.dense(dropout2, 128, activation=tf.nn.relu, name=\"hidden3\",use_bias=True, kernel_initializer= he_init,\n",
    "                                      bias_initializer=he_init)\n",
    "        dropout3= tf.layers.dropout(hidden3, rate=0.5,name=\"dropout3\")\n",
    "        \n",
    "        hidden4 = tf.layers.dense(dropout3, 128, activation=tf.nn.relu, name=\"hidden4\",use_bias=True, kernel_initializer= he_init,\n",
    "                                      bias_initializer=he_init)\n",
    "        dropout4= tf.layers.dropout(hidden4, rate=0.5,name=\"dropout4\")\n",
    "        \n",
    "        hidden5 = tf.layers.dense(dropout4, 128, activation=tf.nn.relu, name=\"hidden5\",use_bias=True, kernel_initializer= he_init,\n",
    "                                      bias_initializer=he_init)\n",
    "        dropout5= tf.layers.dropout(hidden5, rate=0.5,name=\"dropout5\")\n",
    "            \n",
    "        # 結合之後的 tf.nn.sparse_softmax_cross_entropy_with_logits [128 , 5]\n",
    "        logits = tf.layers.dense(dropout5, n_class, name=\"logits\")\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def Train_op(y, logits,batch_size,n_train):\n",
    "    with tf.name_scope(\"calc_loss\"):\n",
    "        entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(entropy, name=\"loss\")\n",
    "    \n",
    "    ## 此區使用AdamOptimizer 的優化器進行梯度優化\n",
    "    with tf.name_scope(\"train\"):\n",
    "        batch = tf.Variable(0)\n",
    "\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                1e-4,  # Base learning rate.\n",
    "                batch * batch_size,  # Current index into the dataset.\n",
    "                n_train,  # Decay step.\n",
    "                0.95,  # Decay rate.\n",
    "                staircase=True)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(0.001)\n",
    "        training_op = optimizer.minimize(loss, global_step=batch,name=\"training_op\")\n",
    "\n",
    "    return (loss, training_op)\n",
    "\n",
    "def acc_model(y, logits):\n",
    "    #計算正確率   \n",
    "    with tf.name_scope('calc_accuracy'):\n",
    "        correct = tf.equal(tf.argmax(logits, 1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32),name=\"accuracy\")\n",
    "\n",
    "    #計算precision 因返回會有兩個值，只取後者\n",
    "    with tf.name_scope(\"precision\"):\n",
    "        _, precision = tf.metrics.precision(predictions = tf.argmax(logits,1), labels=y)\n",
    "\n",
    "    #計算recall 因返回會有兩個值，只取後者\n",
    "    with tf.name_scope('recall'):\n",
    "        _, recall = tf.metrics.recall(predictions = tf.argmax(logits,1), labels=y)\n",
    "\n",
    "    return (accuracy, precision, recall)\n",
    "\n",
    "\n",
    "def shuffle_data(data, labels):\n",
    "    idx = np.random.permutation(len(data))\n",
    "    data, label = data[idx], labels[idx]\n",
    "    return (data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training DNN model\n",
    "def get_Dnn_model(total_pl):\n",
    "    \n",
    "    NUM_PL = 8\n",
    "    D_WORD = 300\n",
    "    FOLDS = 5\n",
    "    \n",
    "    x, y = pl_preprocessing(total_pl)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    ###### test 同 training data #######\n",
    "    X_train, X_test1, Y_train, y_test1 = train_test_split(x, y, test_size = 0.2)\n",
    "    data = CrossValidationFolds(X_train, Y_train, FOLDS)\n",
    "    (X_train1, y_train1), (X_valid1, y_valid1) = data.split()\n",
    "    \n",
    "    print(X_train1.shape,y_train1.shape)\n",
    "\n",
    "    ###### test 不同 training data #######\n",
    "    # data = CrossValidationFolds(x, y, FOLDS)\n",
    "    # (X_train1, y_train1), (X_valid1, y_valid1) = data.split()\n",
    "\n",
    "    # X_test1,y_test1 = load_pl('../new_Steeve_data/filter_Dice/can/')\n",
    "    # X_test1 = np.array(X_test1)\n",
    "    # y_test1 = np.array(y_test1)\n",
    "    \n",
    "    ### 先前設置 \n",
    "    \n",
    "    in_units = D_WORD*NUM_PL\n",
    "    n_class = 6 # 題目要求只要辨識 0 ,1 ,2 ,3 及4 ，共5個類別\n",
    "\n",
    "    n_train = len(X_train1) # train資料的長度\n",
    "    batch_size = 50\n",
    "    n_batch = n_train // batch_size\n",
    "\n",
    "    X = tf.placeholder(tf.float32,[None,in_units],name=\"X\") # 初始化x資料型態為[None,784]\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # 初始化y資料型態[None]\n",
    "    \n",
    "    logits = L_layers_model(X, 128, n_class, 0.5)\n",
    "    Y_proba=tf.nn.softmax(logits,name=\"Y_proba\")\n",
    "    loss, train_op = Train_op(y, logits,batch_size,n_train)\n",
    "    accuracy, precision, recall = acc_model(y, logits)\n",
    "\n",
    "    prediction=tf.argmax(Y_proba,1)\n",
    "\n",
    "    saver = tf.train.Saver()  # call save function\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 1}) #指定gpu\n",
    "    \n",
    "    # Params for Train\n",
    "    epochs = 1000 # 10 for augmented training data, 20 for training data\n",
    "    val_step = 100 # 當 50 步時去算一次驗證資料的正確率\n",
    "\n",
    "    # Training cycle\n",
    "    max_acc = 0. # Save the maximum accuracy value for validation data\n",
    "    early_stop_limit = 0 # 紀錄early_stop的值\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        sess.run([init, init_l])\n",
    "        for epoch in range(epochs):\n",
    "            if early_stop_limit >= 200: \n",
    "                print('early_stop...........')\n",
    "                break\n",
    "\n",
    "            # Random shuffling\n",
    "            train_data, train_label = shuffle_data(X_train1, y_train1)\n",
    "\n",
    "            # 用批次的方式去訓練 model\n",
    "            for i in range(n_batch):\n",
    "            # Compute the offset of the current minibatch in the data.\n",
    "                offset = (i * batch_size) % (n_train)\n",
    "                batch_xs = train_data[offset:(offset + batch_size), :]\n",
    "                batch_ys = train_label[offset:(offset + batch_size)]\n",
    "                sess.run([train_op, loss], feed_dict={X:batch_xs, y: batch_ys})\n",
    "\n",
    "                # 每 n step時，model去看此時的驗證資料的正確率並印出來\n",
    "                if i % val_step == 0:\n",
    "                    val_acc = sess.run(accuracy, feed_dict={X: X_valid1, y: y_valid1})\n",
    "                    print(\"Epoch:\", '%04d,' % (epoch + 1),\n",
    "                          \"batch_index %4d/%4d , validation accuracy %.5f\" % (i, n_batch, val_acc))\n",
    "\n",
    "                # 透過最大驗證正確率大於每一次的驗證正確率的條件來設定 early stop\n",
    "                    if max_acc >= val_acc:\n",
    "                        early_stop_limit += 1\n",
    "                        if early_stop_limit == 200: # 自己可以去限制最大驗證正確率不再變n次時，就停止訓練\n",
    "                            break\n",
    "\n",
    "                # 如果 val_acc 大於 max_acc，則取代它並儲存一次結果\n",
    "                    else: # validation_accuracy > max_acc\n",
    "                        early_stop_limit = 0\n",
    "                        max_acc = val_acc\n",
    "                        saver.save(sess,DNN_model_ck_path)                        \n",
    "                        print('dnn_model.ckpt-' + 'complete-%04d-' % (epoch + 1) + \n",
    "                          \"batch_index-%d\" % i)\n",
    "        sess.run(init_l)\n",
    "        saver.restore(sess,DNN_model_ck_path) # 開啟剛剛 early_stop 的 model\n",
    "\n",
    "        print('Acc_test :' , sess.run(accuracy, feed_dict={X: X_test1, y: y_test1}))\n",
    "        print('Prec_value :' , sess.run(precision, feed_dict={X: X_test1, y: y_test1}))\n",
    "        print('Recall_value :' , sess.run(recall, feed_dict={X: X_test1, y: y_test1}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_raw_pl():\n",
    "     \n",
    "    print(\"load data\")\n",
    "    total_data = [] \n",
    "    r = requests.get('https://steevebot.ml/all')\n",
    "    ori_data = r.json()\n",
    "    key = list(ori_data.keys())\n",
    "    \n",
    "    for k in key: \n",
    "        data = []\n",
    "    \n",
    "        for num, job_num in enumerate(ori_data[k]):\n",
    "            if num%500 ==0:\n",
    "                print(num)\n",
    "\n",
    "            pl_des = get_pl_keywords(job_num[\"jobDescription\"])\n",
    "            pl_ski = get_pl_keywords(job_num[\"skills\"])\n",
    "            data.append(pl_des+pl_ski)\n",
    "\n",
    "        total_data.append(data)\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Job PL, and predict a field\n",
    "def get_predict_field(pl_job,pl_cnt):\n",
    "    NUM_PL = 8\n",
    "    D_WORD = 300\n",
    "\n",
    "    x = get_pl_v(pl_job,pl_cnt,NUM_PL,D_WORD)\n",
    "    x = np.array(x)\n",
    "    x = x.reshape([1,-1])\n",
    "    \n",
    "    restore_saver = tf.train.import_meta_graph(DNN_model_meta_path)\n",
    "    X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "    y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "    loss = tf.get_default_graph().get_tensor_by_name(\"calc_loss/loss:0\")\n",
    "    Y_proba = tf.get_default_graph().get_tensor_by_name(\"Y_proba:0\")\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        prediction=tf.argmax(Y_proba,1)\n",
    "        sess.run(init)\n",
    "#         print(\"predictions\", prediction.eval(feed_dict={X: x}, session=sess),Ty)\n",
    "        \n",
    "        predict_field = prediction.eval(feed_dict={X: x}, session=sess)\n",
    "\n",
    "    return predict_field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trainig DNN model\n",
    "\n",
    "total_pl = get_all_raw_pl()\n",
    "get_Dnn_model(total_pl)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Predict field\n",
    "\n",
    "get_predict_field(pl of job)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
