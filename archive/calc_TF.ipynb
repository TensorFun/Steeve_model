{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/vincent/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from collections import Counter, defaultdict\n",
    "from functools import reduce\n",
    "import json\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "from pyfasttext import FastText\n",
    "model = FastText('../wiki.en.bin')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_extraction, svm, metrics\n",
    "from sklearn.grid_search import GridSearchCV  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfidf(doc, file):\n",
    "    vectorizer  = CountVectorizer(min_df=1, token_pattern=r'[\\w\\+\\.#-]+')  # 該類會將文本中的詞語轉換為詞頻矩陣，矩陣元素a[i][j] 表示j詞在i類文本下的詞頻  \n",
    "    transformer = TfidfTransformer()                                       # 該類會統計每個詞語的tf-idf權值  \n",
    "    tfidf = transformer.fit_transform(vectorizer.fit_transform(doc))       # 第一個fit_transform是計算tf-idf，第二個fit_transform是將文本轉為詞頻矩陣  \n",
    "    \n",
    "    words  = vectorizer.get_feature_names()                                # 獲取詞袋模型中的所有詞語  \n",
    "    weight = tfidf.toarray()                                               # 將tf-idf矩陣抽取出來，元素a[i][j]表示j詞在i類文本中的tf-idf權重  \n",
    "    \n",
    "    tfidf_score = defaultdict(lambda: defaultdict())\n",
    "    for i in range(len(weight)):                                           # 打印每類文本的tf-idf詞語權重，第一個for遍歷所有文本，第二個for便利某一類文本下的詞語權重  \n",
    "#         print(\"-------這裡輸出第\",i,u\"類文本的詞語tf-idf權重------\")\n",
    "        for j in range(len(words)):\n",
    "#             tfidf_score[i][words[j]] = weight[i][j]\n",
    "            tfidf_score[index_to_field[i]][words[j]] = weight[i][j]\n",
    "#             print(words[j], weight[i][j])\n",
    "            \n",
    "#     with open(file, 'w', encoding='utf8') as ws:\n",
    "#         ws.write(json.dumps(tfidf_score))\n",
    "    return tfidf_score, words\n",
    "\n",
    "def norm_pls(pls):\n",
    "    return [pl.lower().replace(' ', '_').replace('.js', '') for pl in pls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data(directory, limit=None):\n",
    "    X, y, index_to_field, field_to_index = [], [], {}, {}\n",
    "    files = [file for file in listdir(directory) if not file.startswith(\".\")]\n",
    "    for i, file in enumerate(files):\n",
    "        doc = json.loads(open(directory + file, 'r', encoding='utf8').read())\n",
    "        posts = sum(doc.values(), [])\n",
    "        posts = posts[:limit] if limit else posts\n",
    "        print(file, \"posts length: {}\".format(len(posts)))\n",
    "        \n",
    "        index_to_field[i], field_to_index[file] = file, i\n",
    "        for post in posts:\n",
    "            X.append(norm_pls(post['PL']))\n",
    "            y.append(i)\n",
    "    return X, y, index_to_field, field_to_index\n",
    "\n",
    "def get_doc(X, y): # for tfidf\n",
    "    doc = defaultdict(lambda: [])\n",
    "    for pls, field in zip(X, y):\n",
    "        doc[field] += pls\n",
    "    \n",
    "    doc = [' '.join(doc[i]) for i in range(len(doc))]\n",
    "    return doc\n",
    "        \n",
    "def tfidf_predict(test_X):\n",
    "    _y, unexist = [], Counter()\n",
    "    for i, post in enumerate(test_X):\n",
    "        _y.append(get_possible_field(post, unexist))\n",
    "    print(unexist)\n",
    "    return _y\n",
    "\n",
    "def get_possible_field(post, unexist=Counter()):\n",
    "    scores = defaultdict(lambda: 0.0)\n",
    "    pls = norm_pls(post)\n",
    "    for pl in pls:\n",
    "        if pl not in words:\n",
    "            unexist[pl] += 1\n",
    "            continue\n",
    "                \n",
    "        for field, score_table in tfidf_score.items():\n",
    "            scores[field_to_index[field]] += score_table[pl]\n",
    "                \n",
    "    try:\n",
    "        index = max(scores.items(), key=operator.itemgetter(1))[0]\n",
    "        return index\n",
    "    except:\n",
    "        return 0 # 隨機猜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_feature(X):\n",
    "    new_X = []\n",
    "    for post in X:\n",
    "        field = get_possible_field(post)\n",
    "        vec = to_vec(field, post)\n",
    "        new_X.append(vec)\n",
    "    return new_X\n",
    "\n",
    "########## MODEL ###########\n",
    "def to_vec(field, post): # field: index\n",
    "    field = index_to_field[field]\n",
    "    top_pl = list(filter(lambda x: x != '' and x in words, post))\n",
    "    top_pl = sorted(top_pl, key=lambda x: tfidf_score[field][x], reverse=True)[:4]\n",
    "\n",
    "    if len(top_pl) == 0:\n",
    "        vec = np.zeros(1200)\n",
    "    else:\n",
    "        top_pl += top_pl[0] + top_pl[0] + top_pl[0] + top_pl[0]\n",
    "        vec = [col for pl in top_pl[:4] for col in model.get_numpy_vector(pl)]\n",
    "    return vec\n",
    "\n",
    "    \n",
    "    \n",
    "#     vec = np.zeros(300)\n",
    "#     post = set(post) # unique\n",
    "#     for pl in post:\n",
    "#         if pl == '': continue\n",
    "#         if pl not in words: continue\n",
    "            \n",
    "#         vec += model.get_numpy_vector(pl) * tfidf_score[index_to_field[field]][pl]\n",
    "#     return vec\n",
    "\n",
    "def train_and_predict(train_X, test_X, train_y, test_y):\n",
    "    # 建立 SVC 模型\n",
    "    svc = svm.SVC(kernel='poly', probability=True, max_iter=100)\n",
    "    svc_fit = svc.fit(train_X, train_y)\n",
    "\n",
    "    # 預測\n",
    "    _y = svc.predict(test_X)\n",
    "    return _y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mfilter_CareerBuilder\u001b[0m/  \u001b[01;34mno_filter_CareerBuilder\u001b[0m/  \u001b[01;34mresume\u001b[0m/\r\n",
      "\u001b[01;34mfilter_Dice\u001b[0m/           \u001b[01;34mno_filter_Dice\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls '/home/fun/Atos/new_Steeve_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_file = 'tfidf_score_cb_filter.json'\n",
    "train = 'no_filter_Dice'\n",
    "test = 'no_filter_CareerBuilder'\n",
    "\n",
    "TRAIN_PATH = '/home/fun/Atos/new_Steeve_data/' + train + '/can/'\n",
    "TEST_PATH = '/home/fun/Atos/new_Steeve_data/' + test + '/can/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywordsandroid.txt posts length: 3600\n",
      "Keywordsbackend.txt posts length: 3540\n",
      "Keywordsfrontend.txt posts length: 3600\n",
      "Keywordspm.txt posts length: 3600\n",
      "Keywordssa.txt posts length: 3600\n",
      "Keywordssecurity.txt posts length: 3600\n",
      "Keywordsandroid.txt posts length: 868\n",
      "Keywordsbackend.txt posts length: 950\n",
      "Keywordsfrontend.txt posts length: 999\n",
      "Keywordspm.txt posts length: 7500\n",
      "Keywordssa.txt posts length: 7500\n",
      "Keywordssecurity.txt posts length: 2500\n",
      "Counter({'methode': 33, 'ms-word': 8, 'observable': 8, 'hugo': 7, 'catwalk': 6, 'nvd3': 4, 'blogger': 4, 'latex': 4, 'duplicates': 4, 'tumblr': 3, 'oracle_commerce': 3, 'discourse': 3, 'pycharm': 2, 'smarty': 2, 'cfml': 2, 'sencha-touch': 2, 'timer': 2, 'prolog': 2, 'adobe_coldfusion': 2, 'quantcast': 2, 'indy': 2, 'activex': 2, 'caddy': 2, 'rcms': 2, 'prospector': 2, 'oracle_application_server': 2, 'virtualenv': 2, 'opengrok': 1, 'afnetworking': 1, 'ios6': 1, 'image-processing': 1, 'apache_wicket': 1, 'codemirror': 1, 'appnexus': 1, 'passport': 1, 'many-to-many': 1, 'exception-handling': 1, 'data-binding': 1, 'activerecord': 1, 'constructor': 1, 'sizmek': 1, 'carousel': 1, 'google_search_appliance': 1, 'oracle11g': 1, 'unicode': 1, 'twiki': 1, 'darwin': 1, 'entity-framework': 1, 'tealeaf': 1, 'sunos': 1, 'tooltip': 1, 'toggle': 1, 'grandstream': 1, 'web-scraping': 1, 'java_servlet': 1, 'clicktale': 1, 'active-directory': 1, 'webs': 1, 'const': 1})\n",
      "0.530344046857\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.49      0.57       868\n",
      "          1       0.24      0.34      0.28       950\n",
      "          2       0.39      0.39      0.39       999\n",
      "          3       0.55      0.85      0.66      7500\n",
      "          4       0.57      0.28      0.37      7500\n",
      "          5       0.59      0.49      0.53      2500\n",
      "\n",
      "avg / total       0.54      0.53      0.50     20317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, index_to_field, field_to_index = get_data(TRAIN_PATH)\n",
    "tfidf_score, words = get_tfidf(get_doc(train_X, train_y), score_file)\n",
    "test_X, test_y, _, _ = get_data(TEST_PATH)\n",
    "_y = tfidf_predict(test_X)\n",
    "\n",
    "print(metrics.accuracy_score(test_y, _y))\n",
    "print(metrics.classification_report(test_y, _y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywordsandroid.txt posts length: 3600\n",
      "Keywordsbackend.txt posts length: 3540\n",
      "Keywordsfrontend.txt posts length: 3600\n",
      "Keywordspm.txt posts length: 3600\n",
      "Keywordssa.txt posts length: 3600\n",
      "Keywordssecurity.txt posts length: 3600\n",
      "Keywordsandroid.txt posts length: 868\n",
      "Keywordsbackend.txt posts length: 950\n",
      "Keywordsfrontend.txt posts length: 999\n",
      "Keywordspm.txt posts length: 7500\n",
      "Keywordssa.txt posts length: 7500\n",
      "Keywordssecurity.txt posts length: 2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=100).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.270364719201\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.00      0.01       868\n",
      "          1       0.21      0.01      0.01       950\n",
      "          2       0.31      0.09      0.14       999\n",
      "          3       0.48      0.18      0.26      7500\n",
      "          4       0.38      0.46      0.42      7500\n",
      "          5       0.07      0.24      0.11      2500\n",
      "\n",
      "avg / total       0.37      0.27      0.27     20317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, index_to_field, field_to_index = get_data(TRAIN_PATH)\n",
    "tfidf_score, words = get_tfidf(get_doc(train_X, train_y), score_file)\n",
    "test_X, test_y, _, _ = get_data(TEST_PATH)\n",
    "\n",
    "train_X = to_feature(train_X)\n",
    "test_X = to_feature(test_X)\n",
    "    \n",
    "_y = train_and_predict(train_X, test_X, train_y, test_y)\n",
    "\n",
    "print(metrics.accuracy_score(test_y, _y))\n",
    "print(metrics.classification_report(test_y, _y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywordsandroid.txt posts length: 3600\n",
      "Keywordsbackend.txt posts length: 3540\n",
      "Keywordsfrontend.txt posts length: 3600\n",
      "Keywordspm.txt posts length: 3600\n",
      "Keywordssa.txt posts length: 3600\n",
      "Keywordssecurity.txt posts length: 3600\n",
      "Keywordsandroid.txt posts length: 868\n",
      "Keywordsbackend.txt posts length: 950\n",
      "Keywordsfrontend.txt posts length: 999\n",
      "Keywordspm.txt posts length: 7500\n",
      "Keywordssa.txt posts length: 7500\n",
      "Keywordssecurity.txt posts length: 2500\n",
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-58a2f3417904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     clf = GridSearchCV(svm.SVC(C=1), tuned_parameters, cv=5,  \n\u001b[1;32m     20\u001b[0m                        scoring='%s_weighted' % score)  \n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best parameters set found on development set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m         \"\"\"\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 574\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m                 for train, test in cv)\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1673\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ANACONDA_ENV/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_X, train_y, index_to_field, field_to_index = get_data(TRAIN_PATH)\n",
    "tfidf_score, words = get_tfidf(get_doc(train_X, train_y), score_file)\n",
    "test_X, test_y, _, _ = get_data(TEST_PATH)\n",
    "\n",
    "train_X = to_feature(train_X)\n",
    "test_X = to_feature(test_X)\n",
    "\n",
    "# Set the parameters by cross-validation  \n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1, 0.1, 0.01, 1e-3, 1e-4], 'C': [0.1, 1, 10]}, \n",
    "                    {'kernel': ['poly'], 'degree': [2, 3, 4, 5, 6, 7, 8], 'coef0': [0, 0.1, 0.01, 1, 10], 'C': [0.1, 1, 10]},\n",
    "                    {'kernel': ['linear'], 'C': [0.1, 1, 10]}]  \n",
    "  \n",
    "scores = ['precision', 'recall']  \n",
    "  \n",
    "for score in scores:  \n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)  \n",
    "    print()  \n",
    "  \n",
    "    clf = GridSearchCV(svm.SVC(C=1), tuned_parameters, cv=5,  \n",
    "                       scoring='%s_weighted' % score)  \n",
    "    clf.fit(train_X, train_y)  \n",
    "  \n",
    "    print(\"Best parameters set found on development set:\")  \n",
    "    print()  \n",
    "    print(clf.best_params_)  \n",
    "    print()  \n",
    "    print(\"Grid scores on development set:\")  \n",
    "    print()  \n",
    "    for params, mean_score, scores in clf.grid_scores_:  \n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"  \n",
    "              % (mean_score, scores.std() * 2, params))  \n",
    "    print()  \n",
    "  \n",
    "    print(\"Detailed classification report:\")  \n",
    "    print()  \n",
    "    print(\"The model is trained on the full development set.\")  \n",
    "    print(\"The scores are computed on the full evaluation set.\")  \n",
    "    print()  \n",
    "    y_true, y_pred = test_y, clf.predict(test_X)  \n",
    "    print(classification_report(y_true, y_pred))  \n",
    "    print()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posts 長度數量不拘\n",
    "\n",
    "#### no_filter / CB\n",
    "* tfidf 傳統法: 0.530344046857\n",
    "* svm\\*tfidf : 0.541763055569\n",
    "\n",
    "#### CB / no_filter\n",
    "* tfidf 傳統法: 0.625858867224\n",
    "* svm\\*tfidf: 0.580501392758\n",
    "\n",
    "#### filter CB / filter Dice\n",
    "* tfidf: 0.732615083252\n",
    "* svm\\*tfidf: 0.758220232265\n",
    "\n",
    "### Posts 長度數量限制\n",
    "\n",
    "#### CB / no_filter 1000:\n",
    "* tfidf: 0.637975858867\n",
    "* svm*tfidf: 0.656360259981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 新 pl filter\n",
    "\n",
    "### 單純 tfidf model\n",
    "\n",
    "* filter / no_filter: 0.7064637452986967\n",
    "* no_filter / filter: 0.9252259654889071\n",
    "\n",
    "### 考慮次數的 vec 相加\n",
    "\n",
    "* filter / no_filter: 0.636753258112\n",
    "* no_fitler / filter: 0.912078882498\n",
    "\n",
    "### 不考慮次數的 vec 相加\n",
    "\n",
    "* filter / no_filter: 0.718883932476\n",
    "* no_fitler / filter: 0.928101889893\n",
    "\n",
    "### 不考慮次數的 vec * tfidf 再相加 （錯誤版）\n",
    "\n",
    "* filter / no_filter: 0.917694393423\n",
    "* no_fitler / filter: 0.965488907149\n",
    "\n",
    "### 不考慮次數的 vec * tfidf 再相加 （正確版）因為並不會取得 label 所以用第一個 model 先預測 field 再做相乘\n",
    "\n",
    "* filter / no_filter: 0.704364558733\n",
    "* no_fitler / filter: 0.916187345933\n",
    "\n",
    "\n",
    "================================\n",
    "\n",
    "# 舊 pl filter\n",
    "\n",
    "### 單純 tfidf model\n",
    "* filter / no_filter 0.699\n",
    "* no_filter / filter 0.9227608874281019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "user_pls = ['JavaScript', 'html', 'vue.js', 'css']\n",
    "field = 'Keywordsfrontend.txt'\n",
    "index = field_to_index[field]\n",
    "\n",
    "content = json.loads(open('/home/fun/Atos/new_Steeve_data/filter_Dice/can/' + field, 'r', encoding='utf8').read())\n",
    "posts = sum(content.values(), [])\n",
    "\n",
    "user_pls = norm_pls(user_pls)\n",
    "user_vec = to_vec(field_to_index[field], user_pls)\n",
    "\n",
    "# posts_vec = [to_vec(index, norm_pls(post['PL'])) for post in posts]   \n",
    "# distances = cosine_similarity([user_vec], posts_vec).squeeze()\n",
    "#  / len(set(norm_pls(post['PL'])))\n",
    "distances = [len(set(user_pls).intersection(set(norm_pls(post['PL'])))) for post in posts]\n",
    "\n",
    "sim = sorted(zip(posts, distances), key=lambda pair: pair[1], reverse=True)\n",
    "sim[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit = 1299\n",
    "score_file = 'tfidf_score_cb.json'\n",
    "\n",
    "X, y, index_to_field, field_to_index = get_data('/home/fun/Atos/new_Steeve_data/CareerBuilder/can/', limit)\n",
    "print(index_to_field)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.2)\n",
    "print(\"Training data length: {}, Test data length: {}\".format(len(train_X), len(test_X)))\n",
    "tfidf_score, words = get_tfidf(get_doc(train_X, train_y), score_file)\n",
    "\n",
    "###### 傳統 tfidf 法\n",
    "_y = tfidf_predict(test_X)\n",
    "print(\"tfidf model\")\n",
    "print(metrics.accuracy_score(test_y, _y))\n",
    "print(metrics.classification_report(test_y, _y))\n",
    "\n",
    "###### svm * tfidf\n",
    "train_X = to_feature(train_X)\n",
    "test_X = to_feature(test_X)\n",
    "\n",
    "_y = train_and_predict(train_X, test_X, train_y, test_y)\n",
    "print(\"SVM + tfidf model\")\n",
    "print(metrics.accuracy_score(test_y, _y))\n",
    "print(metrics.classification_report(test_y, _y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
