{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, OrderedDict\n",
    "import os\n",
    "from operator import itemgetter    \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from TFIDF import TFIDF\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfasttext import FastText\n",
    "model = FastText('/home/vincent/atos/wiki.en.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pl_v(j):\n",
    "    m = []\n",
    "    aaa = {}\n",
    "    \n",
    "    # compare four fields tf-idf value\n",
    "    N_SCORE = .0\n",
    "    MAX_SCORE = .0\n",
    "    PREDICT_FIELD = '' \n",
    "    \n",
    "    for cnt_f in pl_cnt:\n",
    "        for pl in j:\n",
    "            if pl_cnt[cnt_f].get(pl) == None:\n",
    "                continue\n",
    "                N_SCORE += pl_cnt[cnt_f].get(pl)\n",
    "            if N_SCORE > MAX_SCORE:\n",
    "                PREDICT_FIELD = cnt_f\n",
    "                MAX_SCORE = N_SCORE\n",
    "            elif PREDICT_FIELD == '':\n",
    "                PREDICT_FIELD = cnt_f\n",
    "                        \n",
    "    for pl in j:\n",
    "#         print(type(pl))\n",
    "        if pl_cnt[PREDICT_FIELD].get(pl) == None:\n",
    "            pass\n",
    "        else:\n",
    "            aaa[pl] = pl_cnt[PREDICT_FIELD].get(pl)\n",
    "            \n",
    "    for i, g in sorted(dict(aaa).items(), key=itemgetter(1), reverse=True)[:8]:\n",
    "        try:\n",
    "            x = list(model[i])\n",
    "            m = m + x\n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "                \n",
    "    if len(m) < D_WORD*NUM_PL:\n",
    "        if (len(m)/D_WORD)%2 == 1:\n",
    "            m = m+m[0:D_WORD]\n",
    "        if len(m) == D_WORD*6:\n",
    "            m = m + m[0:D_WORD*2]\n",
    "        elif len(m) == D_WORD*4:\n",
    "            m = m + m\n",
    "        elif len(m) == D_WORD*2:\n",
    "#                 m = m + m[0:D_WORD]\n",
    "            m = m + m + m + m\n",
    "#         if len(m) != D_WORD*NUM_PL:\n",
    "#           pass\n",
    "#                 print('no',len(m))\n",
    "#     else:\n",
    "#         print('yes',len(m))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl_preprocessing(total_pl):\n",
    "    train_data = []\n",
    "    train_y = []\n",
    "    \n",
    "    # label\n",
    "    l = 0\n",
    "    \n",
    "    for field in total_pl:\n",
    "#         print(field)\n",
    "        for num, j in enumerate(field):\n",
    "\n",
    "            m = get_pl_v(j)\n",
    "            if len(m) == 2400:                    \n",
    "                train_data.append(get_pl_v(j))\n",
    "                train_y.append(l)\n",
    "            else:\n",
    "                pass\n",
    "        l += 1\n",
    "#                             print(i)\n",
    "#     print(t,s)\n",
    "            \n",
    "    return train_data,train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 額外程式，主要去打亂合併的資料，並做k-fold 的取資料\n",
    "class CrossValidationFolds(object):\n",
    "    \n",
    "    def __init__(self, data, labels, num_folds, shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.num_folds = num_folds\n",
    "        self.current_fold = 0\n",
    "        \n",
    "        # Shuffle Dataset\n",
    "        if shuffle:\n",
    "            perm = np.random.permutation(self.data.shape[0]) ##隨機打亂資料\n",
    "            data = data[perm]\n",
    "            labels = labels[perm]\n",
    "    \n",
    "    def split(self):\n",
    "        current = self.current_fold\n",
    "        size = int(self.data.shape[0]/self.num_folds) # 30596 / 5 一塊k的size大小\n",
    "        \n",
    "        index = np.arange(self.data.shape[0]) \n",
    "\n",
    "        # 利用 True/False 抓出 validation 區塊\n",
    "        lower_bound = index >= current*size # validation 下界\n",
    "        upper_bound = index < (current + 1)*size # 上界\n",
    "\n",
    "        cv_region = lower_bound*upper_bound\n",
    "\n",
    "        cv_data = self.data[cv_region] # 利用 True/False 抓出 True 的資料\n",
    "        train_data = self.data[~cv_region]\n",
    "        \n",
    "        cv_labels = self.labels[cv_region]\n",
    "        train_labels = self.labels[~cv_region]\n",
    "        \n",
    "        self.current_fold += 1 ## 丟回下一的fold\n",
    "        return (train_data, train_labels), (cv_data, cv_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layers_model(X, h_units, n_class, dropout=0.5):\n",
    "    # default he_init: factor=2.0, mode='FAN_IN', uniform=False, seed=None, dtype=tf.float32\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    with tf.name_scope(\"DNN\"):\n",
    "        hidden1 = tf.layers.dense(X, 128, activation=tf.nn.relu, name=\"hidden1\",use_bias=True, kernel_initializer= he_init,\n",
    "                                      bias_initializer=he_init)\n",
    "        dropout1= tf.layers.dropout(hidden1, rate=0.5,name=\"dropout1\")\n",
    "        \n",
    "        hidden2 = tf.layers.dense(dropout1, 128, activation=tf.nn.relu, name=\"hidden2\",use_bias=True, kernel_initializer= he_init,\n",
    "                                      bias_initializer=he_init)\n",
    "        dropout2= tf.layers.dropout(hidden2, rate=0.5,name=\"dropout2\")\n",
    "        \n",
    "        hidden3 = tf.layers.dense(dropout2, 128, activation=tf.nn.relu, name=\"hidden3\",use_bias=True, kernel_initializer= he_init,\n",
    "                                      bias_initializer=he_init)\n",
    "        dropout3= tf.layers.dropout(hidden3, rate=0.5,name=\"dropout3\")\n",
    "        \n",
    "        hidden4 = tf.layers.dense(dropout3, 128, activation=tf.nn.relu, name=\"hidden4\",use_bias=True, kernel_initializer= he_init,\n",
    "                                      bias_initializer=he_init)\n",
    "        dropout4= tf.layers.dropout(hidden4, rate=0.5,name=\"dropout4\")\n",
    "        \n",
    "        hidden5 = tf.layers.dense(dropout4, 128, activation=tf.nn.relu, name=\"hidden5\",use_bias=True, kernel_initializer= he_init,\n",
    "                                      bias_initializer=he_init)\n",
    "        dropout5= tf.layers.dropout(hidden5, rate=0.5,name=\"dropout5\")\n",
    "            \n",
    "        # 結合之後的 tf.nn.sparse_softmax_cross_entropy_with_logits [128 , 5]\n",
    "        logits = tf.layers.dense(dropout5, n_class, name=\"logits\")\n",
    "    \n",
    "    return logits\n",
    "\n",
    "def Train_op(y, logits):\n",
    "    with tf.name_scope(\"calc_loss\"):\n",
    "        entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(entropy, name=\"loss\")\n",
    "    \n",
    "    ## 此區使用AdamOptimizer 的優化器進行梯度優化\n",
    "    with tf.name_scope(\"train\"):\n",
    "        batch = tf.Variable(0)\n",
    "\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                1e-4,  # Base learning rate.\n",
    "                batch * batch_size,  # Current index into the dataset.\n",
    "                n_train,  # Decay step.\n",
    "                0.95,  # Decay rate.\n",
    "                staircase=True)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(0.001)\n",
    "        training_op = optimizer.minimize(loss, global_step=batch,name=\"training_op\")\n",
    "\n",
    "    return (loss, training_op)\n",
    "\n",
    "def acc_model(y, logits):\n",
    "    #計算正確率   \n",
    "    with tf.name_scope('calc_accuracy'):\n",
    "        correct = tf.equal(tf.argmax(logits, 1), y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32),name=\"accuracy\")\n",
    "\n",
    "    #計算precision 因返回會有兩個值，只取後者\n",
    "    with tf.name_scope(\"precision\"):\n",
    "        _, precision = tf.metrics.precision(predictions = tf.argmax(logits,1), labels=y)\n",
    "\n",
    "    #計算recall 因返回會有兩個值，只取後者\n",
    "    with tf.name_scope('recall'):\n",
    "        _, recall = tf.metrics.recall(predictions = tf.argmax(logits,1), labels=y)\n",
    "\n",
    "    return (accuracy, precision, recall)\n",
    "\n",
    "\n",
    "def shuffle_data(data, labels):\n",
    "    idx = np.random.permutation(len(data))\n",
    "    data, label = data[idx], labels[idx]\n",
    "    return (data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(sess, X_train1, y_train1, X_valid1, y_valid1):\n",
    "    early_stop_limit = 0\n",
    "    max_acc = 0.\n",
    "        \n",
    "    sess.run([init, init_l])\n",
    "    summary_writer = tf.summary.FileWriter('tensorbord/', graph=tf.get_default_graph())\n",
    "    # 可以自己選擇大概要幾個epoch，因為我們有early stop的方法，所以不怕一直train\n",
    "    for epoch in range(epochs):\n",
    "        if early_stop_limit >= 200: \n",
    "            print('early_stop...........')\n",
    "            break\n",
    "\n",
    "        # Random shuffling\n",
    "        train_data, train_label = shuffle_data(X_train1, y_train1)\n",
    "\n",
    "        # 用批次的方式去訓練 model\n",
    "        for i in range(n_batch):\n",
    "            # Compute the offset of the current minibatch in the data.\n",
    "            offset = (i * batch_size) % (n_train)\n",
    "            batch_xs = train_data[offset:(offset + batch_size), :]\n",
    "            batch_ys = train_label[offset:(offset + batch_size)]\n",
    "            sess.run([train_op, loss], feed_dict={X:batch_xs, y: batch_ys})\n",
    "\n",
    "            # 每 n step時，model去看此時的驗證資料的正確率並印出來\n",
    "            if i % val_step == 0:\n",
    "                val_acc = sess.run(accuracy, feed_dict={X: X_valid1, y: y_valid1})\n",
    "                print(\"Epoch:\", '%04d,' % (epoch + 1), \n",
    "                      \"batch_index %4d/%4d , validation accuracy %.5f\" % (i, n_batch, val_acc))\n",
    "\n",
    "                # 透過最大驗證正確率大於每一次的驗證正確率的條件來設定 early stop\n",
    "                if max_acc >= val_acc:\n",
    "                    early_stop_limit += 1\n",
    "                    if early_stop_limit == 200: # 自己可以去限制最大驗證正確率不再變n次時，就停止訓練\n",
    "                        break\n",
    "\n",
    "                # 如果 val_acc 大於 max_acc，則取代它並儲存一次結果\n",
    "                else: # validation_accuracy > max_acc\n",
    "                    early_stop_limit = 0\n",
    "                    max_acc = val_acc\n",
    "                    saver.save(sess, '../dnn_model.ckpt')                        \n",
    "                    print('dnn_model.ckpt-' + 'complete-%04d-' % (epoch + 1) + \n",
    "                          \"batch_index-%d\" % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Dnn_model(total_pl):\n",
    "    \n",
    "    NUM_PL = 8\n",
    "    D_WORD = 300\n",
    "    tf_idf = TFIDF(total_pl)\n",
    "    pl_cnt, words = tf_idf.get_tfidf()\n",
    "    FOLDS = 5\n",
    "    \n",
    "    x, y = pl_preprocessing(total_pl,NUM_PL,pl_cnt,D_WORD)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    ###### test 同 training data #######\n",
    "    X_train, X_test1, Y_train, y_test1 = train_test_split(x, y, test_size = 0.2)\n",
    "    data = CrossValidationFolds(X_train, Y_train, FOLDS)\n",
    "    (X_train1, y_train1), (X_valid1, y_valid1) = data.split()\n",
    "    \n",
    "    print(X_train1.shape,y_train1.shape)\n",
    "\n",
    "    ###### test 不同 training data #######\n",
    "    # data = CrossValidationFolds(x, y, FOLDS)\n",
    "    # (X_train1, y_train1), (X_valid1, y_valid1) = data.split()\n",
    "\n",
    "    # X_test1,y_test1 = load_pl('../new_Steeve_data/filter_Dice/can/')\n",
    "    # X_test1 = np.array(X_test1)\n",
    "    # y_test1 = np.array(y_test1)\n",
    "\n",
    "    ##### testing data ######\n",
    "    # Tx = X_test1[0]\n",
    "    # Ty = y_test1[0]\n",
    "    # Tx = Tx.reshape([1,-1])\n",
    "    # print(Tx.shape)\n",
    "    # print(X_test1.shape)\n",
    "    \n",
    "    ### 先前設置 \n",
    "    \n",
    "    in_units = D_WORD*NUM_PL\n",
    "    n_class = 6 # 題目要求只要辨識 0 ,1 ,2 ,3 及4 ，共5個類別\n",
    "\n",
    "    n_train = len(X_train1) # train資料的長度\n",
    "    batch_size = 50\n",
    "    n_batch = n_train // batch_size\n",
    "\n",
    "    X = tf.placeholder(tf.float32,[None,in_units],name=\"X\") # 初始化x資料型態為[None,784]\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # 初始化y資料型態[None]\n",
    "    \n",
    "    logits = L_layers_model(X, 128, n_class, 0.5)\n",
    "    Y_proba=tf.nn.softmax(logits,name=\"Y_proba\")\n",
    "    loss, train_op = Train_op(y, logits,batch_size,n_train)\n",
    "    accuracy, precision, recall = acc_model(y, logits)\n",
    "\n",
    "    prediction=tf.argmax(Y_proba,1)\n",
    "\n",
    "    saver = tf.train.Saver()  # call save function\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 1}) #指定gpu\n",
    "    \n",
    "    # Params for Train\n",
    "    epochs = 1000 # 10 for augmented training data, 20 for training data\n",
    "    val_step = 100 # 當 50 步時去算一次驗證資料的正確率\n",
    "\n",
    "    # Training cycle\n",
    "    max_acc = 0. # Save the maximum accuracy value for validation data\n",
    "    early_stop_limit = 0 # 紀錄early_stop的值\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        run(sess, X_train1, y_train1, X_valid1, y_valid1,init, init_l)\n",
    "        sess.run(init_l)\n",
    "        saver.restore(sess, '../dnn_model.ckpt') # 開啟剛剛 early_stop 的 model\n",
    "\n",
    "        print('Acc_test :' , sess.run(accuracy, feed_dict={X: X_test1, y: y_test1}))\n",
    "        print('Prec_value :' , sess.run(precision, feed_dict={X: X_test1, y: y_test1}))\n",
    "        print('Recall_value :' , sess.run(recall, feed_dict={X: X_test1, y: y_test1}))\n",
    "    \n",
    "\n",
    "    ######## Predict label #########\n",
    "#     print(\"predictions\", prediction.eval(feed_dict={X: Tx}, session=sess),Ty)\n",
    "#     pre = sess.run(tf.argmax(logits, 1), feed_dict={X: X_test1})\n",
    "#     print(classification_report(y_test1.ravel(), pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_field(cv):\n",
    "    \n",
    "    ######## Predict label #########\n",
    "#     print(\"predictions\", prediction.eval(feed_dict={X: Tx}, session=sess),Ty)\n",
    "#     pre = sess.run(tf.argmax(logits, 1), feed_dict={X: X_test1})\n",
    "#     print(classification_report(y_test1.ravel(), pre))\n",
    "    \n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_raw_pl():\n",
    "     \n",
    "    print(\"load data\")\n",
    "    total_data = [] \n",
    "    r = requests.get('https://steevebot.ml/all')\n",
    "    ori_data = r.json()\n",
    "    key = list(ori_data.keys())\n",
    "    \n",
    "    for k in key: \n",
    "        data = []\n",
    "    \n",
    "        for num, job_num in enumerate(ori_data[k]):\n",
    "            if num%500 ==0:\n",
    "                print(num)\n",
    "\n",
    "            pl_des = get_pl_keywords(job_num[\"jobDescription\"])\n",
    "            pl_ski = get_pl_keywords(job_num[\"skills\"])\n",
    "            data.append(pl_des+pl_ski)\n",
    "\n",
    "        total_data.append(data)\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "2400 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fun/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, batch_index    0/  94 , validation accuracy 0.28571\n",
      "dnn_model.ckpt-complete-0001-batch_index-0\n",
      "Epoch: 0002, batch_index    0/  94 , validation accuracy 0.64835\n",
      "dnn_model.ckpt-complete-0002-batch_index-0\n",
      "Epoch: 0003, batch_index    0/  94 , validation accuracy 0.60862\n",
      "Epoch: 0004, batch_index    0/  94 , validation accuracy 0.65765\n",
      "dnn_model.ckpt-complete-0004-batch_index-0\n",
      "Epoch: 0005, batch_index    0/  94 , validation accuracy 0.64582\n",
      "Epoch: 0006, batch_index    0/  94 , validation accuracy 0.65511\n",
      "Epoch: 0007, batch_index    0/  94 , validation accuracy 0.66695\n",
      "dnn_model.ckpt-complete-0007-batch_index-0\n",
      "Epoch: 0008, batch_index    0/  94 , validation accuracy 0.66272\n",
      "Epoch: 0009, batch_index    0/  94 , validation accuracy 0.63736\n",
      "Epoch: 0010, batch_index    0/  94 , validation accuracy 0.66695\n",
      "Epoch: 0011, batch_index    0/  94 , validation accuracy 0.64328\n",
      "Epoch: 0012, batch_index    0/  94 , validation accuracy 0.65934\n",
      "Epoch: 0013, batch_index    0/  94 , validation accuracy 0.63905\n",
      "Epoch: 0014, batch_index    0/  94 , validation accuracy 0.65342\n",
      "Epoch: 0015, batch_index    0/  94 , validation accuracy 0.62975\n",
      "Epoch: 0016, batch_index    0/  94 , validation accuracy 0.62384\n",
      "Epoch: 0017, batch_index    0/  94 , validation accuracy 0.61369\n",
      "Epoch: 0018, batch_index    0/  94 , validation accuracy 0.63567\n",
      "Epoch: 0019, batch_index    0/  94 , validation accuracy 0.65258\n",
      "Epoch: 0020, batch_index    0/  94 , validation accuracy 0.65596\n",
      "Epoch: 0021, batch_index    0/  94 , validation accuracy 0.65850\n",
      "Epoch: 0022, batch_index    0/  94 , validation accuracy 0.63567\n",
      "Epoch: 0023, batch_index    0/  94 , validation accuracy 0.62806\n",
      "Epoch: 0024, batch_index    0/  94 , validation accuracy 0.65173\n",
      "Epoch: 0025, batch_index    0/  94 , validation accuracy 0.64751\n",
      "Epoch: 0026, batch_index    0/  94 , validation accuracy 0.63652\n",
      "Epoch: 0027, batch_index    0/  94 , validation accuracy 0.63905\n",
      "Epoch: 0028, batch_index    0/  94 , validation accuracy 0.63652\n",
      "Epoch: 0029, batch_index    0/  94 , validation accuracy 0.64835\n",
      "Epoch: 0030, batch_index    0/  94 , validation accuracy 0.63314\n",
      "Epoch: 0031, batch_index    0/  94 , validation accuracy 0.64666\n",
      "Epoch: 0032, batch_index    0/  94 , validation accuracy 0.63483\n",
      "Epoch: 0033, batch_index    0/  94 , validation accuracy 0.64243\n",
      "Epoch: 0034, batch_index    0/  94 , validation accuracy 0.63483\n",
      "Epoch: 0035, batch_index    0/  94 , validation accuracy 0.63990\n",
      "Epoch: 0036, batch_index    0/  94 , validation accuracy 0.64328\n",
      "Epoch: 0037, batch_index    0/  94 , validation accuracy 0.63398\n",
      "Epoch: 0038, batch_index    0/  94 , validation accuracy 0.63145\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-74bd1e607f95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../dnn_model.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 開啟剛剛 early_stop 的 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-991e044ba43e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(sess, X_train1, y_train1, X_valid1, y_valid1)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mbatch_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m# 每 n step時，model去看此時的驗證資料的正確率並印出來\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from DNN_data import get_Dnn_model\n",
    "from pl_module import get_pl_keywords\n",
    "\n",
    "total_data = get_all_raw_pl()\n",
    "NUM_PL = 8\n",
    "D_WORD = 300\n",
    "tf_idf = TFIDF(total_data)\n",
    "pl_cnt, words = tf_idf.get_tfidf()\n",
    "FOLDS = 5\n",
    "    \n",
    "x, y = pl_preprocessing(total_data)\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "    ###### test 同 training data #######\n",
    "X_train, X_test1, Y_train, y_test1 = train_test_split(x, y, test_size = 0.2)\n",
    "data = CrossValidationFolds(X_train, Y_train, FOLDS)\n",
    "(X_train1, y_train1), (X_valid1, y_valid1) = data.split()\n",
    "print(len(X_train1[1]),y_train1[0])\n",
    "\n",
    "    ### 先前設置 \n",
    "    \n",
    "in_units = D_WORD*NUM_PL\n",
    "n_class = 6 # 題目要求只要辨識 0 ,1 ,2 ,3 及4 ，共5個類別\n",
    "\n",
    "n_train = len(X_train1) # train資料的長度\n",
    "batch_size = 50\n",
    "n_batch = n_train // batch_size\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,in_units],name=\"X\") # 初始化x資料型態為[None,784]\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # 初始化y資料型態[None]\n",
    "    \n",
    "logits = L_layers_model(X, 128, n_class, 0.5)\n",
    "Y_proba=tf.nn.softmax(logits,name=\"Y_proba\")\n",
    "loss, train_op = Train_op(y, logits)\n",
    "accuracy, precision, recall = acc_model(y, logits)\n",
    "\n",
    "prediction=tf.argmax(Y_proba,1)\n",
    "\n",
    "saver = tf.train.Saver()  # call save function\n",
    "config = tf.ConfigProto(device_count = {'GPU': 1}) #指定gpu\n",
    "    \n",
    "\n",
    "epochs = 1000 # 10 for augmented training data, 20 for training data\n",
    "val_step = 100 # 當 50 步時去算一次驗證資料的正確率\n",
    "\n",
    "    # Training cycle\n",
    "max_acc = 0. # Save the maximum accuracy value for validation data\n",
    "early_stop_limit = 0 # 紀錄early_stop的值\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "    \n",
    "with tf.Session(config=config) as sess:\n",
    "    run(sess, X_train1, y_train1, X_valid1, y_valid1)\n",
    "    sess.run(init_l)\n",
    "    saver.restore(sess, '../dnn_model.ckpt') # 開啟剛剛 early_stop 的 model\n",
    "    \n",
    "    print('Acc_test :' , sess.run(accuracy, feed_dict={X: X_test1, y: y_test1}))\n",
    "    print('Prec_value :' , sess.run(precision, feed_dict={X: X_test1, y: y_test1}))\n",
    "    print('Recall_value :' , sess.run(recall, feed_dict={X: X_test1, y: y_test1}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
