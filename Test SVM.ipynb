{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Clean Merged and Seperate PL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from pl_module import get_pl_keywords, all_pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ['jobID', 'jobTitle', 'jobEmployer', 'jobLocation', 'jobPostTime', 'skills', 'employmentType', 'baseSalary', 'jobDescription', 'url']\n",
    "def read_dir_json(raw_dir, extract_entry='jobDescription', limit=None):\n",
    "    '''\n",
    "    iterate all files under this dir path and retrieve each jobDescription\n",
    "    \n",
    "    return { file: [[], [], []] }, each file (field) and its posts' jobDescription\n",
    "    '''\n",
    "    files_content = defaultdict(lambda: [])\n",
    "\n",
    "    for i, file in enumerate(os.listdir(raw_dir)):\n",
    "        if file.startswith(\".\"): continue # skip .ipynb_checkpoints/\n",
    "\n",
    "        content = json.loads(open(raw_dir + file).read())\n",
    "        all_posts = sum(content.values(), []) # concat all posts (dict.values())\n",
    "        all_posts = all_posts[:limit] if limit else all_posts\n",
    "        \n",
    "        print(file, len(all_posts))\n",
    "        for post in all_posts:\n",
    "            keywords = list(map(lambda el: el.replace(' ', '_'), get_pl_keywords(post[extract_entry])))\n",
    "            files_content[file].append(keywords) # 保留每個領域的每個 post 結構及順序 { field: [[], [], []] }\n",
    "            \n",
    "    return files_content\n",
    "\n",
    "def to_data(raw):\n",
    "    X, y, labels = [], [], {}\n",
    "    for i, field in enumerate(raw):\n",
    "        labels[i] = field\n",
    "        for each in raw[field]:\n",
    "            X.append(each)\n",
    "            y.append(i)\n",
    "    return X, y, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyfasttext import FastText\n",
    "model = FastText('../wiki.en.bin')\n",
    "\n",
    "def get_ctr(X, y):\n",
    "    total_ctr, fields_ctr = Counter(), defaultdict(lambda: Counter())\n",
    "    for x, label in zip(X, y):\n",
    "        ctr = Counter(x)\n",
    "        fields_ctr[label] += ctr\n",
    "        total_ctr += ctr\n",
    "    return total_ctr, fields_ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate Data, Train and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(X, y, ratio=0.2):\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "    print(\"Training data length: {}, Test data length: {}\".format(len(train_X), len(test_X)))\n",
    "    \n",
    "    return train_X, test_X, train_y, test_y\n",
    "\n",
    "def train_and_predict(train_X, test_X, train_y, test_y):\n",
    "    # 建立 SVC 模型\n",
    "    svc = svm.SVC()\n",
    "    temp_X = list(map(lambda el: el[\"vec\"], train_X))\n",
    "    svc_fit = svc.fit(temp_X, train_y)\n",
    "\n",
    "    # 預測\n",
    "    temp_X = list(map(lambda el: el[\"vec\"], test_X))\n",
    "    _y = svc.predict(temp_X)\n",
    "\n",
    "    # 準確\n",
    "    acc = metrics.accuracy_score(test_y, _y)\n",
    "    print(acc)\n",
    "    \n",
    "    return _y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def core(X, y, total_ctr, fields_ctr):\n",
    "    DIM = 300\n",
    "    \n",
    "    def to_vec(tokens, label):    \n",
    "        vec = np.zeros(DIM)\n",
    "        tokens = set(tokens) ###\n",
    "        for i, t in enumerate(tokens):\n",
    "            if t == '': continue\n",
    "#             if t not in total_ctr: \n",
    "#                 print(t)\n",
    "#                 continue ###\n",
    "\n",
    "            ### model core\n",
    "            vec += model.get_numpy_vector(t)\n",
    "#             vec += (model.get_numpy_vector(t) * fields_ctr[label][t] / total_ctr[t])\n",
    "        return vec\n",
    "    \n",
    "    return list(map(lambda pair: {\"vec\": to_vec(pair[0], pair[1]), \"src\": pair[0]}, zip(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Raw_dir = \"./Raw_Data/no_filter/\"\n",
    "Raw_filter_dir = \"./Raw_Data/filter/\"\n",
    "Career_dir = \"./Raw_Data/career_builder/\"\n",
    "\n",
    "Data_dir = \"./Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend.all.json 3300\n",
      "Front-End-Developer.nofilter.NY.json 3630\n",
      "android.2294.version3.json 2294\n",
      "security.2622.version2.json 2622\n",
      "Training data length: 9476, Test data length: 2370\n",
      "0.737130801688\n"
     ]
    }
   ],
   "source": [
    "# filter or no_filter 自切自測\n",
    "raw_contents = read_dir_json(Raw_dir)\n",
    "X, y, labels = to_data(raw_contents)\n",
    "\n",
    "train_X, test_X, train_y, test_y = split_data(X, y)\n",
    "\n",
    "total_ctr, fields_ctr = get_ctr(train_X, train_y)\n",
    "\n",
    "vec_train_X = core(train_X, train_y, total_ctr, fields_ctr)\n",
    "vec_test_X = core(test_X, test_y, total_ctr, fields_ctr)\n",
    "\n",
    "_y = train_and_predict(vec_train_X, vec_test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Security.filter.jobtitle.withid.json 776\n",
      "andriod.filter.jobtitle.withid.json 433\n",
      "Backend.filter.jobtitle.withid.json 394\n",
      "Frontend.filter.jobtitle.withid.json 831\n",
      "Backend.all.json 3300\n",
      "Front-End-Developer.nofilter.NY.json 3630\n",
      "android.2294.version3.json 2294\n",
      "security.2622.version2.json 2622\n",
      "thymeleaf\n",
      "Drupal_Commerce\n",
      "sprite\n",
      "Craft_CMS\n",
      "react-redux\n",
      "Webix\n",
      "Unbounce\n",
      "styled_components\n",
      "sprite\n",
      "jekyll\n",
      "Sitefinity\n",
      "thymeleaf\n",
      "screenshot\n",
      "Bulma\n",
      "thymeleaf\n",
      "Quill\n",
      "Kentico_CMS\n",
      "react-redux\n",
      "Chartjs\n",
      "styled_components\n",
      "Craft_CMS\n",
      "Webix\n",
      "styled_components\n",
      "IBM_Coremetrics\n",
      "Marionette.js\n",
      "thymeleaf\n",
      "0.0160230073952\n"
     ]
    }
   ],
   "source": [
    "# 同時用 filter and no_filter\n",
    "raw_filter_contents = read_dir_json(Raw_filter_dir)\n",
    "raw_contents = read_dir_json(Raw_dir)\n",
    "\n",
    "train_X, train_y, labels = to_data(raw_contents)\n",
    "test_X, test_y, labels = to_data(raw_filter_contents)\n",
    "total_ctr, fields_ctr = get_ctr(train_X, train_y)\n",
    " \n",
    "# 0.0743710957285 raw filter\n",
    "# 0.0160230073952\n",
    "vec_train_X = core(train_X, train_y, total_ctr, fields_ctr)\n",
    "vec_test_X = core(test_X, test_y, total_ctr, fields_ctr)\n",
    "\n",
    "_y = train_and_predict(vec_train_X, vec_test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 career builder data 自切自測\n",
    "raw_contents = read_dir_json(Career_dir, 'job_Description')\n",
    "X, y, labels = to_data(raw_contents)\n",
    "\n",
    "train_X, test_X, train_y, test_y = split_data(X, y)\n",
    "\n",
    "total_ctr, fields_ctr = get_ctr(train_X, train_y)\n",
    "\n",
    "vec_train_X = core(train_X, train_y, total_ctr, fields_ctr)\n",
    "vec_test_X = core(test_X, test_y, total_ctr, fields_ctr)\n",
    "\n",
    "_y = train_and_predict(vec_train_X, vec_test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同時用 career builder and no_filter\n",
    "raw_contents1 = read_dir_json(Career_dir, 'job_Description')\n",
    "raw_contents2 = read_dir_json(Raw_filter_dir)\n",
    "\n",
    "train_X, train_y, labels = to_data(raw_contents1)\n",
    "test_X, test_y, labels = to_data(raw_contents2)\n",
    "total_ctr, fields_ctr = get_ctr(train_X, train_y)\n",
    "\n",
    "vec_train_X = core(train_X, train_y, total_ctr, fields_ctr)\n",
    "vec_test_X = core(test_X, test_y, total_ctr, fields_ctr)\n",
    "\n",
    "_y = train_and_predict(vec_train_X, vec_test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Rank\n",
    "1. 單純將所有 keywords vec 相加：0.84\n",
    "2. 各自 post 內的 keywords 做比例分配當作 weight ，再做相加：0.82~0.79 // 理論上和第一個方法同個概念\n",
    "3. 各自 post 內的 keywords 做統計後以 pagerank x^-4/3 做計算： 0.84~0.82\n",
    "\n",
    "### 不考慮 keywords 出現次數： keyword vector * (field_ctr[word] / total_ctr[word])\n",
    "4. data: filter，並切 20%，0.98\n",
    "5. data: no_filter，並切 20%，0.89\n",
    "\n",
    "6. train: filter,    test: no_filter，0.36\n",
    "7. train: no_filter, test: filter，0.27\n",
    "\n",
    "8. data: career_builder，並切 20%，七個領域，0.82\n",
    "9. data: career_builder，並切 20%，七個領域取同樣數量，0.85\n",
    "\n",
    "10. train: career,    test: no_filter，0.33\n",
    "11. train: career,    test: filter，0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_name(string):\n",
    "    return string.split(\".\")[0]\n",
    "\n",
    "for i, val in enumerate(_y):\n",
    "    if val == test_y[i]: continue\n",
    "\n",
    "    print(\"Answer: {}, Predict: {}\\n{}\\n\".format(get_name(labels[test_y[i]]), get_name(labels[val]), vec_test_X[i][\"src\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
